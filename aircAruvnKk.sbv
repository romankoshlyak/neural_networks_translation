0:00:04.020,0:00:10.680
Це трійка. Вона недбало написана і відображена із надзвичайно низькою роздільною здатністю - 28х28 пікселів.

0:00:10.680,0:00:15.660
Але ваш мозок без проблем розпізнає її як трійку. Я хочу, щоб ви зупинились на мить і оцінили:

0:00:15.900,0:00:18.949
наскільки це надзвичайно, що мозок може зробити це без особливих зусиль!

0:00:18.949,0:00:23.160
Я маю на увазі, що це, це, і ось це також розпізнається як трійка,

0:00:23.160,0:00:28.060
навіть якщо конкретні значення кожного пікселя сильно відрізняються від одного зображення до іншого.

0:00:28.080,0:00:33.780
Конкретні світлочутливі клітини у вашому оці, які активуються, коли ви бачите цю трійку,

0:00:33.780,0:00:36.800
значно відрізняються від тих, що активуються, коли ви бачите ось цю трійку.

0:00:37.140,0:00:40.610
Але щось у вашій неймовірно розумній зоровій корі

0:00:41.129,0:00:48.139
вирішує, що вони представляють одну і ту ж саму ідею, водночас розпізнаючи інші зображення як зовсім інші ідеї.

0:00:48.840,0:00:55.039
Але що коли я сказав би вам: сядьте і напишіть програму, яка приймає на вхід сітку 28х28 пікселів,

0:00:55.379,0:01:01.759
як ось ця, і виводить одне число від 0 до 10, вгадуючи його?

0:01:02.250,0:01:06.139
В такому разі завдання переходить від комічно тривіального до неймовірно складного

0:01:06.750,0:01:08.270
Якщо ви не жили у печері,

0:01:08.270,0:01:14.599
я думаю, навряд чи потрібно пояснювати актуальність та важливість машинного навчання та нейронних мереж для сьогодення і майбутнього

0:01:14.640,0:01:18.410
Але що я хочу зробити тут - це показати вам, що таке, власне, нейромережа, 

0:01:18.660,0:01:24.220
припускаючи, що ви не маєте про це уявлення, і допомогти візуалізувати як вона працює, не як модне слівце, а з точки зору математики

0:01:24.560,0:01:28.300
Я сподіваюся, що по закінченні ви матимете уявлення, чим обумовлена її структура,

0:01:28.380,0:01:34.400
і знатимете, що це означає, коли читатимете або чутимете про так зване "тренування нейромереж"

0:01:34.950,0:01:40.249
Це відео якраз буде присвячене структурі нейромережі, а наступне розповість про навчання

0:01:40.530,0:01:45.950
Що ми будемо робити - це створювати нейронну мережу, яка може навчитися розпізнавати рукописні цифри

0:01:49.270,0:01:51.329
Це доволі класичний приклад

0:01:51.520,0:01:56.759
для знайомства з даною темою, і я радий зберегти статус-кво, тому що наприкінці двох відео я хочу

0:01:56.760,0:02:02.099
вказати вам на кілька хороших ресурсів, де ви можете дізнатися більше, завантажити код, який це робить, і погратися з ним

0:02:02.100,0:02:04.100
на власному комп’ютері

0:02:04.750,0:02:08.970
Існує безліч варіантів нейронних мереж, і в останні роки

0:02:08.970,0:02:11.970
стався цілий бум у дослідженнях цих варіантів

0:02:12.130,0:02:19.019
Але в цих двох вступних відео ми з вами просто збираємося подивитися на найпростішу форму, без жодних додаткових прикрас

0:02:19.300,0:02:21.040
Це свого роду необхідна передумова

0:02:21.040,0:02:24.510
для розуміння будь-якого з більш потужних сучасних варіантів,

0:02:24.760,0:02:28.199
і повірте мені, вона все одно достатньо складна для розуміння

0:02:28.690,0:02:32.820
Але навіть у цій найпростішій формі мережа може навчитися розпізнавати рукописні цифри,

0:02:32.820,0:02:36.180
що досить круто як для комп'ютера.

0:02:37.120,0:02:41.960
І в той же час ви побачите, як вона не виправдовує деякі наші надії

0:02:43.090,0:02:48.179
Як випливає з назви, нейронні мережі отримали назву від мозку, але давайте з'ясуємо,

0:02:48.520,0:02:51.389
що таке нейрони і як вони пов'язані між собою?

0:02:52.090,0:02:57.750
Саме зараз, коли я кажу "нейрон", єдине, про що я хочу, щоб ви подумали, - це щось, що містить у собі число

0:02:58.200,0:03:02.120
Конкретно - число між 0 і 1. І це дійсно не є чимось більшим.

0:03:03.420,0:03:11.120
Наприклад, мережа починається з групи нейронів, які співвідносяться з кожним із 28х28 пікселів вхідного зображення,

0:03:11.400,0:03:12.460
що складає

0:03:12.460,0:03:20.240
784 нейронів загалом. Кожен з них містить число, яке представляє відтінок сірого у відповідному пікселі

0:03:20.769,0:03:24.299
в діапазоні від 0 для чорних пікселів до 1 для білих.

0:03:24.910,0:03:29.240
Це число всередині нейрона називається його активацією.

0:03:29.240,0:03:33.959
Ви можете уявити, що кожен нейрон загоряється, коли містить більше число. 

0:03:36.260,0:03:41.559
Отже, всі ці 784 нейрони складають 
перший шар нашої мережі.

0:03:45.990,0:03:51.289
Тепер переходимо до останнього шару: в ньому є десять нейронів, кожен з яких позначає одну цифру

0:03:51.570,0:03:56.239
активація в цих нейронах - знову ж таки, деяке число, яке знаходиться між нулем і одиницею - 

0:03:56.880,0:04:00.049
представляє, наскільки система вважає, що дане зображення

0:04:00.720,0:04:05.990
співвідноситься з відповідною цифрою. Також є кілька шарів усередині, які називаються прихованими

0:04:06.180,0:04:07.770
і які на даний момент

0:04:07.770,0:04:13.549
мають бути просто одним великим знаком питання - як взагалі цей процес розпізнавання цифр буде працювати

0:04:13.740,0:04:20.209
У цій мережі я вибрав два прихованих шари, кожен з 16 нейронів, і, правду кажучи, це довільний вибір

0:04:20.609,0:04:24.889
Якщо чесно, я вибрав два шари, виходячи з того, як я хочу пояснити структуру через хвилину,

0:04:25.350,0:04:29.179
а 16 - просто таку кількість зручно розташувати на екрані

0:04:29.180,0:04:32.209
На практиці тут є багато місця для експериментів із конкретною структурою

0:04:32.730,0:04:38.329
Те, як мережа здійснює активації в одному шарі, визначає активації наступного рівня

0:04:38.760,0:04:44.060
І звичайно, суть мережі як механізму обробки інформації зводиться до того, 

0:04:44.200,0:04:48.409
як саме активації з одного шару викликають активації в наступному шарі

0:04:48.900,0:04:54.859
Згрубша це можна прирівняти до того, 
як у біологічних нейромережах спрацьовують деякі групи нейронів,

0:04:55.400,0:04:57.400
викликаючи спрацьовування інших нейронів.

0:04:57.560,0:04:58.340
Тепер мережа,

0:04:58.340,0:05:03.020
яку я показую тут, вже навчена розпізнавати цифри, і дозвольте мені показати, що я маю на увазі

0:05:03.140,0:05:06.580
Це означає, що якщо ви подаєте зображення, висвітлюючи всі

0:05:06.640,0:05:11.780
784 нейронів вхідного шару відповідно до яскравості кожного пікселя на зображенні,

0:05:12.330,0:05:17.029
такий шаблон активацій приведе до певного, дуже специфічного шаблону у наступному шарі,

0:05:17.190,0:05:19.309
який зумовить наступний шаблон,

0:05:19.440,0:05:22.190
що, зрештою, дасть певний шаблон у вихідному шарі, 

0:05:22.350,0:05:29.359
і найяскравіший нейрон цього вихідного шару - це вибір мережі, так би мовити, цифра, яку представляє це зображення

0:05:32.070,0:05:36.859
І перш ніж зануритися в математику того, як один шар впливає на наступний чи як працює тренування,

0:05:37.140,0:05:43.069
давайте просто поговоримо про те, 
чому це взагалі має сенс - очікувати, що 
така багатошарова структура буде поводитися розумно

0:05:43.800,0:05:48.260
Чого ми тут очікуємо? Що в кращому випадку можуть зробити ці середні шари?

0:05:48.860,0:05:56.720
Власне, коли ви чи я розпізнаємо цифри, ми з'єднуємо різні компоненти: дев'ятка має петлю вгорі та лінію праворуч;

0:05:57.260,0:06:01.280
8 також має петлю вгорі, але вона поєднується з іншою петлею внизу;

0:06:02.020,0:06:06.599
4 в основному розбивається на три конкретні лінії тощо.

0:06:07.180,0:06:11.970
Тепер в ідеалі ми могли б сподіватися, що кожен нейрон у передостанньому шарі

0:06:12.640,0:06:14.729
відповідає одному з цих підкомпонентів;

0:06:14.890,0:06:19.740
що кожного разу, коли ви подаєте зображення з петлею вгорі, як у 9 або 8,

0:06:19.870,0:06:21.220
є конкретний нейрон,

0:06:21.220,0:06:27.749
активація якого буде близькою до одиниці. І я не маю на увазі цей конкретний набір пікселів: очікується, що

0:06:28.090,0:06:35.039
будь-яка петля у верхній частині активує цей нейрон. Таким чином, перехід від третього шару до останнього

0:06:35.380,0:06:39.960
лише вимагає знати, яка комбінація підкомпонентів відповідає конкретній цифрі.

0:06:40.510,0:06:42.810
Звичайно, це просто відкладає проблему на потім,

0:06:42.910,0:06:49.019
бо як можна розпізнати ці підкомпоненти або хоча б дізнатися, якими мають бути правильні підкомпоненти? І це ще я навіть не говорив про те,

0:06:49.020,0:06:52.829
як один шар впливає на наступний. 
Але давайте розглянемо таке:

0:06:53.650,0:06:56.340
розпізнавання петлі також може розбиватися на підзавдання.

0:06:56.860,0:07:02.550
Одним з обґрунтованих способів зробити це було б спочатку розпізнати різні маленькі грані, з яких вона утворена.

0:07:03.520,0:07:08.910
Аналогічно довга лінія, яку ви можете бачити в цифрах 1, 4 або 7 - 

0:07:08.910,0:07:14.279
це дійсно просто довга грань, або можна вважати її певним шаблоном з декількох менших граней.

0:07:14.740,0:07:19.379
Тож, можливо, ми сподіваємось, що кожен нейрон у другому шарі мережі

0:07:20.290,0:07:22.650
відповідає різним маленьким граням

0:07:23.230,0:07:28.259
Можливо, коли таке зображення з'являється, воно активує всі нейрони,

0:07:28.720,0:07:31.649
пов'язані приблизно з 8-10 конкретними маленькими гранями,

0:07:31.930,0:07:36.930
що, у свою чергу, активує нейрони, пов'язані з верхньою петлею і довгою вертикальною лінією,

0:07:37.300,0:07:39.599
і вони активують нейрон, пов'язаний з дев'яткою.

0:07:40.300,0:07:41.100
Так чи інакше,

0:07:41.100,0:07:47.070
це і є те, що насправді робить наша мережа в кінці - це ще одне питання, до якого я повернусь, коли побачимо, як тренувати мережу

0:07:47.350,0:07:52.170
Але це все, на що ми можемо сподіватися. Своєрідна ціль для такої багатошарової структури.

0:07:53.020,0:07:59.340
Більше того: уявіть, наскільки можливість виявляти подібні грані і шаблони була б корисною для інших завдань з розпізнавання зображень

0:07:59.740,0:08:06.749
І це не лише розпізнавання зображень: є безліч інтелектуальних задач, які можна розбити на абстрактні шари

0:08:07.690,0:08:14.670
Наприклад, розуміння мови включає отримання необробленого звуку та виділення чітких звуків, які комбінуються для утворення певних складів,

0:08:15.070,0:08:19.829
які поєднуються, утворюючи слова, які поєднують, щоб скласти фрази та більш абстрактні думки тощо.

0:08:20.770,0:08:25.710
Та повернімось до того, як щось із цього насправді працює. Уявіть прямо зараз,

0:08:25.710,0:08:30.449
як саме активації в одному шарі можуть визначати активації в наступному.

0:08:30.670,0:08:35.879
Мета - створити механізм, який міг би об'єднати пікселі в грані,

0:08:35.880,0:08:41.430
або грані в шаблони, чи шаблони у цифри, і зблизька розглянути його на одному дуже конкретному прикладі.

0:08:41.950,0:08:44.189
Скажімо, ми сподіваємося, що один конкретний

0:08:44.380,0:08:50.430
нейрон у другому шарі визначить, містить зображення в цій області грань чи ні

0:08:50.950,0:08:54.960
Питання полягає в тому, які параметри повинна мати мережа

0:08:55.270,0:09:02.490
які з них ви маєте налаштувати так, щоб цього було достатньо, аби потенційно зафіксувати цей шаблон,

0:09:02.590,0:09:07.290
або будь-який інший піксельний шаблон, або шаблон, на якому кілька граней можуть утворити петлю тощо

0:09:08.290,0:09:15.389
Що ми зробимо - це надамо вагу кожному зв’язку між нашим нейроном та нейронами з першого шару

0:09:15.850,0:09:17.850
Ці ваги - це просто числа

0:09:18.190,0:09:25.590
Потім візьмемо усі активації з першого шару і обчислимо їх зважену суму відповідно до цих ваг

0:09:27.370,0:09:31.680
Я вважаю, корисно уявляти ці ваги як організовані в невелику власну сітку

0:09:31.680,0:09:37.079
І я буду використовувати зелені пікселі для позначення позитивних ваг, а червоні пікселі - для негативних,

0:09:37.240,0:09:41.670
де яскравість цього пікселя є деяким вільним відображенням ваги

0:09:42.400,0:09:45.840
Тепер, якщо ми зробимо нульовими ваги, пов'язані практично з усіма пікселями,

0:09:46.150,0:09:49.079
за винятком деяких позитивних ваг в області, яка нас цікавить,

0:09:49.480,0:09:51.310
тоді отримання зваженої суми

0:09:51.310,0:09:57.690
всіх значень пікселів насправді обчислюватиметься додаванням значень пікселя в області, яка нас цікавить

0:09:58.870,0:10:04.440
І якщо ви дійсно хочете, щоб мережа вирішила, чи є тут грань, можна взяти деякі негативні ваги,

0:10:04.900,0:10:06.900
пов'язані з оточуючими пікселями

0:10:07.020,0:10:12.660
Тоді сума буде найбільшою, коли середні пікселі будуть яскравими, а оточуючі - темнішими

0:10:14.280,0:10:18.180
При обчисленні такої зваженої суми можна отримати будь-яке число,

0:10:18.240,0:10:23.180
але для цієї мережі ми хочемо, щоб активації були деякими значеннями від 0 до 1

0:10:23.730,0:10:26.599
тому загальноприйнято є загнати цю зважену суму

0:10:26.910,0:10:32.000
у якусь функцію, яка втисне всі числа у діапазон між 0 і 1

0:10:32.190,0:10:37.249
Звичайна функція, яка робить це, 
називається сигмоїдною функцією; 
також вона відома як логістична крива

0:10:37.980,0:10:43.339
В основному дуже негативні входи виявляються близькими до нуля, дуже позитивні - близькими до 1

0:10:43.339,0:10:46.398
і вона неухильно зростає на вхідних значеннях близько 0

0:10:49.080,0:10:56.029
Тож активація нейрона тут в основному вимірюється тим, наскільки позитивною є відповідна зважена сума

0:10:57.450,0:11:01.819
Але, можливо, ви не хочете, щоб нейрон активізувався, коли зважена сума більша за 0

0:11:02.100,0:11:06.260
Можливо, ви хочете, щоб він був активним лише тоді, коли сума більша, ніж, припустимо, 10

0:11:06.630,0:11:10.279
Тобто вам потрібне деяке зміщення, щоб нейрон був неактивним

0:11:10.860,0:11:16.099
Що ми робимо тоді: просто додаємо якесь інше число - наприклад, мінус 10 - до цієї зваженої суми

0:11:16.529,0:11:19.669
перед тим як передати її в сигмоїдну функцію стиснення

0:11:20.220,0:11:22.730
Це додаткове число називається "зміщенням"

0:11:23.310,0:11:29.060
Отже, ваги визначають, який піксельний шаблон обирає цей нейрон у другому шарі, 

0:11:29.220,0:11:35.450
а зміщення визначає, наскільки великою має бути зважена сума, перш ніж нейрон почне активізуватися

0:11:35.910,0:11:37.910
І це лише один нейрон

0:11:38.120,0:11:41.940
Кожен інший нейрон у цьому шарі буде пов'язаний з усіма

0:11:42.320,0:11:50.620
784 пікселями-нейронами першого шару, і кожне з цих 784 з'єднань має свою власну вагу

0:11:51.330,0:11:57.739
також кожен нейрон має деяке зміщення - якесь інше число, яке ви додаєте до зваженої суми перед тим, як стиснути її сигмоїдою

0:11:58.020,0:12:01.909
І тут є над чим замислитись. З цим прихованим шаром із 16 нейронів

0:12:02.010,0:12:08.270
виходить 784х16 ваг плюс 16 зміщень

0:12:08.490,0:12:14.029
І все це лише з'єднання між першим і другим шаром. З'єднання між іншими шарами

0:12:14.029,0:12:17.208
також мають набір пов'язаних з ними ваг і зміщень

0:12:17.760,0:12:20.680
У підсумку ця мережа має приблизно

0:12:21.280,0:12:23.920
13000 ваг і зміщень

0:12:24.280,0:12:29.540
13000 параметрів, які можна налаштувати, щоб змусити цю мережу діяти по-різному

0:12:30.520,0:12:32.520
Тож коли ми говоримо про навчання,

0:12:32.530,0:12:40.199
мається на увазі, що треба змусити комп'ютер знайти коректні значення для всіх цих чисел,

0:12:40.200,0:12:42.190
щоб це реально вирішувало задачу

0:12:42.190,0:12:43.000
Є один мисленнєвий експеримент,

0:12:43.000,0:12:49.979
водночас цікавий і жахливий: уявіть собі, як ви сидите і визначаєте всі ці ваги та зміщення вручну,

0:12:50.380,0:12:56.159
цілеспрямовано добираючи числа таким чином, щоб другий шар обирав грані, третій шар обирав шаблони тощо.

0:12:56.350,0:13:01.440
Я особисто вважаю це більш доцільним, аніж просто вважати мережу "чорним ящиком",

0:13:01.870,0:13:04.349
бо коли мережа працює не так, як ви очікували,

0:13:04.600,0:13:11.370
якщо ви хоч трохи уявляєте, що ці ваги та зміщення означають, ви маєте точку відліку

0:13:11.680,0:13:16.289
для експериментів з покращенням структури, або коли мережа працює,

0:13:16.290,0:13:18.290
але зовсім не з очікуваних причин,

0:13:18.310,0:13:25.169
то розібратися в тому, що роблять ваги та зміщення - це гарний спосіб кинути виклик власним припущенням

0:13:25.180,0:13:26.350
і дійсно розширити можливості для рішень

0:13:26.350,0:13:30.600
До речі, фактична функція тут трохи громіздка для запису, згодні?

0:13:32.350,0:13:38.460
Тож дозвольте показати вам більш компактний спосіб представлення цих з’єднань. Ось як би ви це бачили,

0:13:38.460,0:13:40.460
якби вирішили прочитати більше про нейронні мережі

0:13:41.110,0:13:45.810
Впорядкуємо всі активації з одного шару у стовпчик як вектор,

0:13:47.470,0:13:52.320
Потім організуємо всі ваги як матрицю, де кожен рядок

0:13:52.900,0:13:57.659
відповідає зв’язкам між одним шаром і певним нейроном у наступному шарі

0:13:58.060,0:14:03.599
Це означає, що отримання зваженої суми активацій першого шару відповідно до цих ваг

0:14:04.000,0:14:09.330
співвідноситься з одним із членів векторного добутку матриці всього, що знаходиться зліва

0:14:13.540,0:14:18.380
До речі, значна частина машинного навчання просто зводиться до доброго розуміння лінійної алгебри

0:14:18.380,0:14:26.940
Отже, тим, хто хоче отримати гарне візуальне пояснення матриць і множення матричного вектора, раджу переглянути серію моїх відео про лінійну алгебру,

0:14:27.250,0:14:28.839
особливо розділ третій

0:14:28.839,0:14:35.759
Повернімося до нашого виразу. Замість того, щоб додавати зміщення до кожного з цих значень окремо, ми зобразимо

0:14:36.010,0:14:42.209
всі ці зміщення у вигляді вектора і додамо весь вектор до попереднього векторного матричного добутку

0:14:42.910,0:14:44.040
Завершальний крок - 

0:14:44.040,0:14:47.250
загорнути це все у сигмоїду.

0:14:47.250,0:14:51.899
Це має означати, що ви 
застосуєте сигмоїдну функцію

0:14:52.420,0:14:54.570
до кожного окремого члена отриманого вектора всередині

0:14:55.510,0:15:00.749
Отже, як тільки ви запишете цю матрицю ваг і ці вектори окремими символами,

0:15:01.000,0:15:07.589
ви зможете виразити весь перехід активацій від одного шару до іншого в дуже компактній, акуратній формі

0:15:07.930,0:15:15.000
Це зробить відповідний код і значно простішим, і швидшим, оскільки багато бібліотек добре оптимізовані під матричне множення

0:15:17.560,0:15:21.359
Пригадуєте, раніше я говорив, що ці нейрони -  це просто об'єкти, які містять число?

0:15:21.790,0:15:26.250
Звичайно, конкретні числа, які вони містять, залежать від зображення, яке ви вводите

0:15:27.790,0:15:32.940
Тож насправді більш точним буде розглядати кожен нейрон як функцію, яка приймає

0:15:33.070,0:15:38.070
виводи усіх нейронів у попередньому шарі і видає число від нуля до одиниці

0:15:38.800,0:15:42.270
Насправді вся мережа є лише функцією, яка приймає

0:15:42.760,0:15:47.010
784 числа як вхід і виводить десять чисел як вихід

0:15:47.470,0:15:48.700
Це абсурдно складна функція,

0:15:48.700,0:15:56.249
яка включає тринадцять тисяч параметрів у вигляді ваг і зміщень, що підбираються за певними шаблонами,

0:15:56.250,0:16:00.270
і яка вимагає багаторазового обчислення матричних векторних добутків та сигмоїдної функції стиснення

0:16:00.610,0:16:06.390
Але це лише функція, і навіть логічно, що вона виглядає складною

0:16:06.390,0:16:12.239
Я маю на увазі, якби вона була простіша, чого б це ми могли сподіватися, що вона зможе вирішити завдання з розпізнавання цифр?

0:16:12.960,0:16:19.559
А як вона вирішує цю задачу? Як ця мережа знаходить відповідні ваги та зміщення, просто переглянувши дані?

0:16:20.080,0:16:26.039
Це я покажу в наступному відео, і я також трохи більше розкажу про те, що насправді робить ця конкретна мережа, яку ми бачимо

0:16:27.130,0:16:32.640
Тепер, я думаю, пора сказати, щоб ви підписалися, щоб отримувати сповіщення, коли з’явиться це відео чи будь-яке інше

0:16:32.760,0:16:37.560
Але насправді більшість із вас не отримує сповіщення від YouTube, чи не так?

0:16:37.560,0:16:42.260
Можливо, чесніше сказати "підпишіться, щоб нейронні мережі, що лежать в основі алгоритму рекомендацій YouTube,

0:16:42.459,0:16:47.639
зрозуміли що ви хочете бачити в рекомендаціях контент цього каналу"

0:16:48.250,0:16:50.250
у будь-якому разі, залишайтеся на зв'язку

0:16:50.410,0:16:53.550
Дуже дякую всім, хто підтримує ці відео на Patreon

0:16:53.589,0:16:56.759
Я трохи повільно працював над серією про ймовірності цього літа

0:16:56.760,0:17:01.379
Але я повернуся до них після цього проекту, тож, меценати, можете шукати оновлення там

0:17:03.310,0:17:05.550
Насамкінець: тут зі мною Ліша Лі,

0:17:05.550,0:17:12.029
чия докторська дисертація була присвячена теоретичним основам глибокого навчання і яка зараз працює в фірмі з венчурним капіталом Amplify Partners

0:17:12.030,0:17:16.530
вона люб'язно забезпечила часткове фінансування цього відео.

0:17:16.530,0:17:19.109
Отже, Ліша, думаю, ми повинні коротко зачепити тему цієї сигмоподібної функції

0:17:19.180,0:17:24.780
Як я розумію, раніше мережі використовували її, щоб стиснути відповідну зважену суму в інтервал від нуля до одиниці

0:17:24.980,0:17:30.340
ніби за аналогією з біологічними нейронами - або неактивними, або активними
(Ліша) - Саме так

0:17:30.360,0:17:36.320
(3B1B) - Але відносно мало сучасних мереж насправді використовують сигмоїди. Це радше стара школа, правда?
(Ліша) - Так, або швидше

0:17:36.370,0:17:42.780
ReLU набагато простіше тренувати
(3B1B) - ReLU означає "випрямлений лінійний блок"

0:17:42.780,0:17:48.839
(Ліша) - Так, це така функція, яка просто бере максимум від 0 і "а", яке дається

0:17:49.120,0:17:53.670
так, як ти пояснював у відео, і я думаю, це було частково навіяно

0:17:54.610,0:17:56.610
біологічною

0:17:56.620,0:17:58.179
аналогією з тим як

0:17:58.179,0:18:03.089
нейрони будуть або активовані, або ні, і якщо буде пройдено певний поріг,

0:18:03.250,0:18:05.250
це буде функція ідентичності

0:18:05.290,0:18:10.439
а якщо ні, він просто не буде активований і дорівнюватиме нулю.
Це своєрідне спрощення,

0:18:10.720,0:18:14.429
використання сигмоїд не допомагало тренувати мережі, або це було дуже важко,

0:18:14.429,0:18:19.589
і в якийсь момент люди просто спробували ReLU, і це спрацювало

0:18:20.110,0:18:22.140
дуже добре для цих неймовірно

0:18:22.690,0:18:25.090
глибоких нейронних мереж.
(3B1B) - Гаразд

0:18:25.090,0:18:26.060
Дякую, Ліша

0:18:26.060,0:18:33.429
Для інформацї: Аmplify Рartners - венчурний капітал раннього етапу, що інвестується в технічних засновників, будуючи наступне покоління компаній, зосереджених на

0:18:33.590,0:18:38.409
застосуванні AI. 
Якщо ви чи хтось, кого ви знаєте, коли-небудь замислювались про те, щоб створити компанію,

0:18:38.450,0:18:43.179
Або якщо ви працюєте на початковому етапі зараз, Amplify Рartners з радістю розглянуть вас.

0:18:43.240,0:18:48.800
Вони навіть створили спеціальну електронну пошту для цього відео 3blue1brown@amplifypartners.com

0:18:48.800,0:18:50.780
тож сміливо звертайтеся до них
