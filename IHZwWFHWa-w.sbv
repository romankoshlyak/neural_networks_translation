0:00:04.070,0:00:07.059
В останньому відео я виклав структуру нейронної мережі

0:00:07.160,0:00:10.089
Тут я подам короткий підсумок, щоб освіжити пам'ять

0:00:10.089,0:00:15.368
А потім у мене є дві основні цілі цього відео. Перша - представити ідею градієнтного спуску,

0:00:15.650,0:00:18.219
що лежить в основі не тільки того, як навчаються нейронні мережі,

0:00:18.220,0:00:20.439
але й того, як працюють багато інших методів машинного навчання

0:00:20.660,0:00:24.609
Після цього ми ще трохи розберемося в тому, як працює ця конкретна мережа

0:00:24.609,0:00:27.758
І що насправді шукають ці приховані шари нейронів

0:00:28.999,0:00:33.489
Нагадування: наша мета - класичний приклад розпізнавання рукописних цифр,

0:00:34.129,0:00:36.129
такий собі "hello world" нейронних мереж

0:00:36.500,0:00:43.090
ці цифри відображаються на сітці 28х28 пікселів, кожен піксель з градацією сірого у значенні між 0 і 1

0:00:43.610,0:00:46.089
Це значення визначає активацію

0:00:46.850,0:00:50.199
784 нейронів у вхідному шарі мережі,

0:00:50.840,0:00:55.719
а далі активація кожного нейрона в наступних шарах базується на зваженій сумі

0:00:56.000,0:01:00.639
усіх активацій у попередньому шарі плюс деяке спеціальне число, яке називається зміщенням

0:01:01.699,0:01:06.338
тоді ви складаєте цю суму за допомогою якоїсь іншої функції - наприклад, сигмоїдного стискання або

0:01:06.400,0:01:08.769
ReLu - так, як я зробив у попередньому відео

0:01:09.110,0:01:15.729
В цілому, враховуючи дещо довільний вибір двох прихованих шарів, з 16 нейронами у кожному,

0:01:16.579,0:01:24.159
мережа має близько 13000 ваг і зміщень, які ми можемо регулювати, і саме ці значення визначають, що, власне, робить ця мережа

0:01:24.799,0:01:28.328
Тоді те, що ми маємо на увазі, говорячи, що ця мережа класифікує задану цифру,

0:01:28.329,0:01:33.429
- що найяскравіший із цих 10 нейронів в останньому шарі відповідає цій цифрі

0:01:33.950,0:01:38.589
Пригадайте: мотивація, яку ми мали на увазі для багатошарової структури, полягала в тому, що, можливо,

0:01:38.780,0:01:44.680
другий шар може розпізнавати грані, третій шар може розпізнавати шаблони - як то петлі та лінії, - 

0:01:44.930,0:01:48.729
а останній може просто поєднувати ці шаблони, щоб розпізнавати цифри

0:01:49.369,0:01:52.029
Отже, тут ми дізнаємось, як навчається мережа

0:01:52.399,0:01:57.099
Нам потрібен алгоритм, за допомогою якого можна показати цій мережі цілий набір навчальних даних,

0:01:57.229,0:02:03.669
який подається у вигляді набору різних зображень рукописних цифр разом із позначками, як вони мають виглядати,

0:02:03.890,0:02:05.659
і це скоригує

0:02:05.659,0:02:09.789
13000 ваг і зміщень таким чином, щоб покращити результат на тренувальний даних

0:02:10.730,0:02:13.569
Сподіваємось, ця багатошарова структура навчиться узагальнювати вивчене 

0:02:14.269,0:02:16.719
і застосовувати до зображень, що виходять за межі тренувальних даних

0:02:16.720,0:02:20.289
Спосіб перевірити це полягає в тому, що після тренування мережі

0:02:20.290,0:02:26.560
ви показуєте їй більше маркованих даних, яких вона ніколи не бачила, і перевіряєте, наскільки точно вона класифікує ці нові зображення

0:02:31.040,0:02:37.000
На щастя для нас, і що робить поширення цього прикладу як базового, добрі люди, що стоять за базою MNIST,

0:02:37.000,0:02:44.289
зібрали колекцію з десятків тисяч рукописних цифрових зображень, кожне з яких позначено цифрами, яким вони відповідають

0:02:44.720,0:02:49.539
Це зухвало саме по собі - говорити, що машина навчається. Коли ви бачите, як це працює,

0:02:49.540,0:02:55.359
це менше скидається на якусь шалену науково-фантастичну задумку і набагато більше схоже на вправи в обчисленнях

0:02:55.390,0:02:59.589
Я маю на увазі, це в основному зводиться до пошуку мінімуму певної функції

0:03:01.519,0:03:05.199
Пам’ятайте: на рівні концепту ми думаємо, що кожен нейрон пов'язаний

0:03:05.390,0:03:12.309
з усіма нейронами попереднього шару, а ваги у зваженій сумі, що визначають його активацію, є свого роду

0:03:12.440,0:03:14.060
мірою сили цих зв'язків

0:03:14.060,0:03:20.440
І зміщення є певним індикатором того, чи цей нейрон є активним або неактивним

0:03:20.440,0:03:26.919
На початку, ми просто ініціалізуємо всі ці ваги та зміщення абсолютно випадковим чином. Зайвим буде говорити, що ця мережа спрацює паршиво

0:03:26.919,0:03:33.759
на даних тренувальних прикладах, оскільки вона просто робитиме щось випадкове: наприклад, ви подаєте це зображення трійки,

0:03:33.760,0:03:35.799
а вихідний шар просто схожий на казна-що

0:03:36.349,0:03:42.518
Отже, що вам треба зробити, то це визначити функцію вартості; це спосіб сказати комп'ютеру: "Ні! Поганий комп'ютер!

0:03:42.739,0:03:50.529
Цей вихід повинен мати активації, які для більшості нейронів дорівнюють нулю, а для цього нейрона - 1, а ти видав мені абсолютну дурницю"

0:03:51.260,0:03:56.530
Говорячи трохи більш математично, ви маєте додати квадрати різниць між

0:03:56.720,0:04:01.419
кожною з цих хибних активацій та значеннями, які вам потрібні

0:04:01.489,0:04:04.599
Це те, що ми назвемо вартістю одного навчального прикладу

0:04:05.599,0:04:10.749
Зауважте, що ця сума невелика, коли мережа впевнено класифікує зображення правильно,

0:04:12.199,0:04:15.639
але вона велика, коли скидається, що мережа не знає, що робить

0:04:18.330,0:04:25.249
Тож наступне, що вам треба зробити, - це визначити середню вартість всіх тих десятків тисяч навчальних прикладів, що є у вашому розпорядженні

0:04:27.060,0:04:34.310
Ця середня вартість є для нас показником того, наскільки паршивою є мережа та як погано почувається комп’ютер, і це справа складна

0:04:34.830,0:04:38.960
Пам'ятаєте, як мережа сама була в основному функцією, яка приймає

0:04:39.540,0:04:45.890
784 числа як вхідні параметри, значення пікселів, і видає десять чисел як вихідні параметри і в певному сенсі

0:04:45.890,0:04:48.770
вона визначається усіма цими вагами та зміщеннями

0:04:49.140,0:04:54.020
В той час як функція вартості є шаром складності поверх цього, вона бере як вхідні параметри

0:04:54.450,0:05:02.059
ці приблизно тринадцять тисяч ваг і зміщень і видає єдине число, що описує, наскільки погані ці ваги і зміщення,

0:05:02.340,0:05:08.749
а спосіб, яким це визначається, залежить від того, як мережа поводиться щодо всіх десятків тисяч одиниць тренувальних даних

0:05:09.150,0:05:11.150
Тут є над чим подумати

0:05:12.000,0:05:15.619
Але просто сказати комп’ютеру, що він робить паршиву роботу, не допомагає

0:05:15.900,0:05:19.819
Ви ж хочете підказати йому, як змінити ваги та зміщення, щоб вони покращились

0:05:20.820,0:05:25.129
Щоб зробити це легшим способом, ніж постаратися уявити функцію з 13000 параметрів,

0:05:25.130,0:05:30.409
уявіть просту функцію, яка має один вхідний параметр і один вихідний

0:05:30.960,0:05:34.999
Як ви знайдете параметр, який мінімізує значення цієї функції?

0:05:36.270,0:05:40.039
Студенти-математики знають, що іноді можна дізнатися цей мінімум точно,

0:05:40.260,0:05:43.879
Але це не завжди можливо для дійсно складних функцій

0:05:44.310,0:05:52.160
Точно не у випадку тринадцятитисячної версії параметра для нашої шалено складної функції вартості нейромережі

0:05:52.350,0:05:59.029
Більш гнучка тактика полягає в тому, щоб почати з будь-якого старого параметра і визначити, в якому напрямку рухатися, щоб понизити вихідний результат

0:06:00.120,0:06:03.710
Зокрема, якщо ви можете дізнатися нахил функції, де ви знаходитесь,

0:06:04.020,0:06:09.619
тоді змістіть параметр ліворуч, якщо цей нахил позитивний, і праворуч - якщо нахил негативний

0:06:12.130,0:06:16.799
Якщо ви зробите це кілька разів у кожній точці, перевіряючи новий нахил і роблячи відповідний крок,

0:06:16.800,0:06:20.039
то наблизитеся до певного локального мінімуму функції

0:06:20.280,0:06:24.080
Зображення, яке ви уявляти, - це кулька, що котиться вниз по схилу 

0:06:24.400,0:06:30.900
Зауважте: навіть для цієї дійсно спрощеної функції з одним параметром є багато можливих "долин", де ви можете приземлитися,

0:06:31.540,0:06:36.220
залежно від того, з якого випадкового параметра ви починаєте. І немає гарантії, що локальний мінімум,

0:06:36.580,0:06:39.040
у якому ви приземляєтесь, буде найменшим можливим значенням функції вартості

0:06:39.610,0:06:44.009
Це також переноситься на наш випадок нейронної мережі. Я також хочу, щоб ви зауважили,

0:06:44.010,0:06:47.190
що, якщо ви робите кроки пропорційними до схилу,

0:06:47.620,0:06:54.540
то коли нахил вирівнюється до мінімуму, кроки стають все меншими і меншими, і це наче вберігає вас від промаху.

0:06:55.720,0:07:00.449
Трохи збільшивши складність, уявіть натомість функцію, яка приймає два вхідні параметри і повертає одне значення

0:07:01.120,0:07:07.739
Ви можете подумати про вхідний простір як про площину XY, а про функцію вартості - як про поверхню, зображену над нею

0:07:08.230,0:07:15.060
Тепер замість того, щоб запитувати про нахил функції, ви повинні запитати, в якому напрямку слід рухатися в цьому вхідному просторі

0:07:15.310,0:07:22.440
так, щоб якнайшвидше зменшити значення функції. Іншими словами, куди саме рухатися вниз?

0:07:22.440,0:07:25.379
І знову ж таки, допомагає подумати про кулю, що котиться по схилу

0:07:26.260,0:07:34.080
Ті з вас, хто знайомий з багатовимірним численням, знають, що градієнт функції дає вам напрям найкрутішого підйому -

0:07:34.750,0:07:38.459
тобто, в якому напрямку вам слід рухатися, щоб найшвидше збільшити функцію 

0:07:39.100,0:07:46.439
Природно, що, приймаючи негативне значення цього градієнта, ви отримуєте напрям для руху, який найшвидше зменшує функцію.

0:07:47.020,0:07:53.400
Навіть більше: довжина цього градієнтного вектора є, власне, індикатором того, наскільки крутим є цей найкрутіший схил

0:07:54.130,0:07:56.280
Тепер, якщо ви незнайомі з багатовимірним численням

0:07:56.280,0:08:00.239
і хочете дізнатися більше, ознайомтеся з роботою, яку я провів для Khan Academy з цієї теми

0:08:00.910,0:08:03.779
Чесно кажучи, єдине, що має значення для нас з вами зараз,

0:08:03.780,0:08:09.419
це чи існує в принципі спосіб обчислити цей вектор. Цей вектор, який вказує вам

0:08:09.520,0:08:15.900
напрям спуску і наскільки він крутий. Цілком достатньо, якщо це все, що ви знаєте, і ви не впевнені в деталях,

0:08:16.790,0:08:24.580
бо якщо ви збагнете, що алгоритм мінімізації функції полягає в обчисленні напрямку градієнта, тоді зробіть маленький крок вниз і

0:08:24.740,0:08:26.740
просто повторюйте це знову і знову

0:08:27.800,0:08:34.600
Це та ж основна ідея для функції, яка має 13000 вхідних параметрів замість двох. Уявіть, що ви організували всі

0:08:35.330,0:08:39.400
13000 ваг і зміщень нашої мережі в гігантський вектор-стовпчик

0:08:39.680,0:08:43.870
Від'ємний градієнт функції витрат - це просто вектор

0:08:43.880,0:08:49.299
Це певний напрямок всередині цього божевільно величезного вхідного простору, який вказує, які

0:08:49.400,0:08:55.030
зміни у всіх цих параметрах приведуть до найшвидшого зниження функції вартості -

0:08:55.460,0:08:58.150
звичайно, з нашою спеціально змодельованою функцією вартості.

0:08:58.580,0:09:04.900
Зміна ваг і зміщень з метою зменшення значення функції означає, що формування результату мережі на кожному елементі тренувальних даних

0:09:05.180,0:09:10.599
менше схоже на випадковий масив з десяти значень і більше нагадуює фактичне рішення, якого ми очікуємо від неї

0:09:11.030,0:09:16.030
Важливо пам’ятати: ця функція вартості включає в себе середнє значення з усіх тренувальних даних

0:09:16.370,0:09:20.590
Тож якщо ви мінімізуєте її, це означатиме покращене виконання усіх цих прикладів

0:09:23.780,0:09:30.849
Алгоритм ефективного обчислення цього градієнта, який є ключем у навчанні нейронної мережі, називається зворотним поширенням.

0:09:31.190,0:09:34.690
І саме про це я буду говорити у наступному відео

0:09:34.690,0:09:36.690
Там я дуже хочу витратити час і пройтися по тому,

0:09:36.830,0:09:41.439
що саме відбувається з кожною вагою і кожним зміщенням для конкретної частини тренувальних даних

0:09:41.810,0:09:46.960
Намагаючись дати інтуїтивне розуміння того, що відбувається поза цією групою відповідних обчислень та формул

0:09:47.510,0:09:52.179
прямо тут і зараз головне, що я хочу, щоб ви знали, що незалежно від деталей реалізації,

0:09:52.180,0:09:58.479
коли говоримо про навчання мереж, то маємо на увазі просто мінімізацію функції вартості

0:09:58.940,0:10:04.479
Зауважте: одним із наслідків цього є те, що для цієї функції витрат важливо мати гарне неперервне значення,

0:10:04.480,0:10:07.810
так щоб ми могли знайти локальний мінімум, рухаючись малими кроками вниз

0:10:08.810,0:10:10.520
Ось чому, до речі,

0:10:10.520,0:10:16.749
у штучних нейронів активації безперервно змінюють значення в межах діапазону, а не є просто активними або неактивними,

0:10:16.750,0:10:18.750
як біологічні нейрони

0:10:19.940,0:10:26.770
Цей процес постійної зміни параметрів функції через певний кратний негативний градієнт називається градієнтним спуском

0:10:26.930,0:10:32.380
Це спосіб звести до деякого локального мінімуму функції вартості, тобто долина на цьому графіку

0:10:32.930,0:10:38.890
Я досі показую зображення функції - з двома вхідними параметрами, звичайно, тому що зміни у 13000-просторовому вхідному просторі

0:10:38.890,0:10:44.049
трохи важко осягнути, але насправді є приємний непросторовий спосіб подумати над цим

0:10:44.630,0:10:51.340
Кожен компонент негативного градієнта повідомляє нам дві речі: знак зазвичай підказує, чи відповідний

0:10:51.830,0:10:59.139
компонент вхідного вектора треба змістити вгору або вниз, але важливо, що відносні величини всіх цих компонентів

0:10:59.840,0:11:02.530
вказуть, які зміни є більш важливими

0:11:05.150,0:11:09.340
Бачите, в нашій мережі зміна параметра однієї ваги може мати набагато більший вплив

0:11:09.710,0:11:12.939
на функцію вартості, ніж зміна параметра якоїсь іншої ваги

0:11:14.450,0:11:17.950
Деякі з цих зв'язків мають більше значення для наших тренувальних даних

0:11:18.920,0:11:22.690
Ось що можна думати про цей градієнтний вектор нашої приголомшливої

0:11:22.690,0:11:27.999
масивної функції вартості: він кодує відносну важливість кожної ваги та зміщення

0:11:28.250,0:11:32.200
тобто яка з цих змін буде найбільш ефективною

0:11:33.560,0:11:36.460
Це дійсно просто інший спосіб думати про напрямок

0:11:36.860,0:11:41.290
Візьмемо простіший приклад: якщо у вас є якась функція з двома змінними в якості вхідних параметрів і ви

0:11:41.690,0:11:46.540
обчислили, що її градієнт у певній точці знаходиться в координатах (3,1)

0:11:47.420,0:11:51.670
Тоді, з одного боку, можна інтерпретувати це так: якщо вважати, що коли ви знаходитеся біля цього вхідного параметра,

0:11:52.070,0:11:55.150
переміщення в цьому напрямку збільшує функцію найшвидше

0:11:55.460,0:12:02.229
Коли ви зображаєте функцію над площиною вхідних точок, саме цей вектор задає вам рух вгору по похилій прямій

0:12:02.600,0:12:06.580
Але ще один спосіб прочитати це - це сказати, що зміни у цій першій змінній

0:12:06.740,0:12:13.390
утричі важливіші, ніж зміни у другій змінній, так що принаймні в області відповідного вхідного параметра

0:12:13.520,0:12:16.689
зміна значення Х є набагато більш ефективною

0:12:19.310,0:12:19.930
Гаразд

0:12:19.930,0:12:24.940
Давайте відступимо на крок і підведемо підсумки, на чому ми стоїмо. Мережа сама по собі є функцією з 

0:12:25.400,0:12:29.859
784 вхідними параметрами і 10 вихідними, що визначаються з урахуванням усіх цих зважених сум

0:12:30.350,0:12:34.780
Функція вартості - це додатковий рівень складності, яка приймає 

0:12:35.120,0:12:41.870
13000 ваг і зміщень як вхідні параметри і повертає результат середньої паршивості на основі тренувальних прикладів,

0:12:42.180,0:12:47.930
Градієнт функції вартості - це ще один рівень складності, який відображає,

0:12:47.930,0:12:53.839
які зміни всіх цих ваг та зміщень викличуть найшвидшу зміну значення функції витрат

0:12:53.970,0:12:57.680
Можна інтерпретувати це, говорячи про те, які зміни для яких ваг є найбільше важливими

0:13:02.550,0:13:09.289
Тож коли ви ініціалізуєте мережу з випадковими вагами та зміщеннями і коригуєте їх багато разів за допомогою градієнтного спуску,

0:13:09.420,0:13:12.949
наскільки добре вона опрацьовує зображення, які раніше ніколи не бачила?

0:13:13.680,0:13:19.609
Мережа, яку я описав тут, з двома прихованими шарами з 16 нейронів кожен, обраними здебільшого з естетичних причин,

0:13:20.579,0:13:26.089
непогана - вона класифікує близько 96% нових зображень правильно,

0:13:26.759,0:13:32.239
Чесно кажучи, якщо ви поглянете на деякі приклади, на яких вона схибила, вам захочеться дати мережі послаблення

0:13:35.759,0:13:39.079
Тепер, якщо ви пограєтеся зі структурою прихованого шару і внесете деякі зміни,

0:13:39.079,0:13:43.698
то зможете покращити результат до 98%, і це дуже добре. Це не найкраще - 

0:13:43.740,0:13:48.409
ви, звісно, можете досягти кращих показників, взявшись за щось складніше, ніж ця звичайна базова мережа

0:13:48.569,0:13:52.669
Але враховуючи, наскільки лякаючим є початкове завдання, я просто думаю, що є щось

0:13:52.889,0:13:56.929
неймовірне у тому, що будь-яка мережа успішно справляється із зображеннями, яких раніше ніколи не бачила,

0:13:57.389,0:14:00.919
враховуючи, що ми ніколи конкретно не говорили їй, на які шаблони слід звернути увагу

0:14:02.579,0:14:07.068
Спочатку я мотивував цю структуру, висловлюючи сподівання,

0:14:07.259,0:14:09.739
що другий шар може вибирати невеликі грані,

0:14:09.809,0:14:17.089
третій шар зможе поєднувати ці грані, щоб розпізнавати петлі та довші лінії, і все це можна буде об'єднати для розпізнавання цифр

0:14:17.699,0:14:22.729
То це і є те, що насправді робить наша мережа? Ну, конкретно наша -

0:14:23.339,0:14:24.449
зовсім ні

0:14:24.449,0:14:27.409
Пригадуєте, як у попередньому відео ми бачили, що ваги

0:14:27.480,0:14:31.849
з'єднань від усіх нейронів першого шару до конкретного нейрона у другому шарі

0:14:31.980,0:14:36.829
можна було візуалізувати як конкретний піксельний шаблон, який міг підбиратися нейроном другого шару?

0:14:37.350,0:14:43.309
Коли ми, власне, робимо це для ваг, пов'язаних із переходами від першого шару до наступного,

0:14:43.709,0:14:50.209
замість того, щоб перебирати окремі маленькі грані то тут, то там - вони здаються майже випадковими - 

0:14:50.370,0:14:56.399
просто помістіть туди кілька дуже розпливчастих шаблонів. Буде здаватися, що в незбагненно великому

0:14:56.920,0:15:02.580
13000-мірному просторі можливих ваг і зміщень наша мережа знайшла собі маленький гарненький локальний мінімум,

0:15:02.860,0:15:08.940
який, незважаючи на успішну класифікацію більшості зображень, не вловлює шаблони, на які ми розраховували.

0:15:09.420,0:15:13.700
Щоб краще в цьому розібратися, подивіться, що відбувається при введенні випадкового зображення

0:15:14.019,0:15:21.449
Якби система була розумною, ви б могли очікувати, що вона почуватиметься невпевнено, можливо, насправді не активує жоден із цих 10 вихідних нейронів

0:15:21.579,0:15:23.200
або активує їх усі рівномірно

0:15:23.200,0:15:24.820
Але натомість

0:15:24.820,0:15:32.010
вона впевнено видає вам якісь безглузді відповіді, наче почуваючись однаково впевнено і щодо того, що це незрозуміле зображення - це 5, 

0:15:32.010,0:15:34.010
і щодо того, що власне зображена 5 - це 5

0:15:34.180,0:15:40.829
Інакше кажучи, навіть якщо ця мережа може розпізнати цифри досить добре, вона не має уявлення, як їх намалювати

0:15:41.500,0:15:45.149
Значною мірою це пов'язано із жорсткою обмеженістю тренувальних налаштувань

0:15:45.149,0:15:51.479
Я маю на увазі, поставте себе на місце мережі - з її точки зору, весь Всесвіт складається ні з чого іншого, 

0:15:51.480,0:15:57.539
як із чітко визначених незмінних цифр, зосереджених на крихітній сітці, і її функція вартості ніколи не спонукала її 

0:15:57.700,0:16:00.959
до чогось іншого, окрім як повністю покладатися на свої рішення

0:16:01.690,0:16:05.070
Тож якщо це зображення того, що реально роблять ці нейрони другого шару,

0:16:05.140,0:16:09.839
Ви можете задатися питанням, чому я представив цю мережу з мотивацією розпізнавання граней і шаблонів

0:16:09.839,0:16:11.969
Я маю на увазі, це не все, що вона в кінцевому підсумку робить

0:16:13.029,0:16:17.909
Для нас це не кінцева мета, а навпаки, пункт відправлення

0:16:17.910,0:16:19.120
Це стара технологія

0:16:19.120,0:16:21.510
досліджувана у 80-х та 90-х

0:16:21.640,0:16:29.129
Вам потрібно зрозуміти її, перш ніж ви зможете зрозуміти більш детальні сучасні варіанти, і це однозначно підходить для вирішення деяких цікавих проблем

0:16:29.410,0:16:34.110
Але чим глибше ви заглиблюєтеся в те, що ці приховані шари справді роблять, тим менш розумним це здається

0:16:38.530,0:16:42.359
Змістімо фокус від того, як навчаються мережі, до того, як навчаєтесь ви:

0:16:42.580,0:16:46.139
це станеться лише в тому випадку, якщо ви активно взаємодіятимете з матеріалом

0:16:46.660,0:16:53.100
Я хочу, щоб ви зробили одну просту річ: зупиніться і задумайтеся на мить про те,

0:16:53.440,0:16:55.230
як ви можете змінити цю систему

0:16:55.230,0:17:00.719
і як вона б розпізнавала зображення, якщо б ви хотіли, щоб вона краще вирізняла грані і шаблони?

0:17:01.360,0:17:04.410
Але ще краще, щоб насправді засвоїти матеріал

0:17:04.410,0:17:05.079
я

0:17:05.079,0:17:08.969
дуже рекомендую книгу Майкла Нільсена про глибоке навчання та нейронні мережі

0:17:09.190,0:17:14.369
У ній ви знайдете код і дані, які можна завантажити і погратися, саме для цього прикладу

0:17:14.410,0:17:18.089
Книга ознайомить вас крок за кроком з тим, що робить цей код

0:17:18.910,0:17:21.749
Прекрасно, що ця книга є безкоштовною та загальнодоступною

0:17:22.360,0:17:27.540
Тож якщо ви скористаєтеся чимось із цього, можете приєднатися до мене і фінансово підтримати Нільсена за його зусилля

0:17:27.910,0:17:32.219
В описі відео я також залишив кілька посилань на інші ресурси, включаючи

0:17:32.470,0:17:36.390
феноменальну і прекрасну публікацію в блозі Кріса Оли та статті на distill

0:17:38.230,0:17:40.200
Щоб закрити цю тему за наступні кілька хвилин,

0:17:40.200,0:17:43.740
Я хочу перестрибнути назад у фрагмент інтерв'ю, яке було у мене з Лішею Лі

0:17:43.930,0:17:49.079
Ви можете пам'ятати її з попереднього відео. Вона написала докторську дисертацію з глибокого навчання, і в цьому маленькому уривку

0:17:49.080,0:17:55.530
вона розповідає про дві останні роботи, які дійсно заглиблюються у те, як, власне, навчаються деякі сучасніші мережі для розпізнавання зображень

0:17:55.810,0:18:01.349
Просто нагадую, на чому ми зупинилися: у першому документі розглядається одна з таких особливо глибоких нейронних мереж,

0:18:01.350,0:18:05.910
якій дійсно добре вдається розпізнавання зображень, і замість того, щоб тренувати її на правильно маркованих даних,

0:18:05.910,0:18:08.579
всі мітки змішали перед тренуванням

0:18:08.800,0:18:14.669
Очевидно, що точність тестування тут мала бути не кращою, ніж випадкова, оскільки все було позначено випадковим чином

0:18:14.800,0:18:20.879
Але мережі все-таки вдалося досягти такої ж точності тренувань, як на даних, маркованих належним чином

0:18:21.490,0:18:27.540
По суті, мільйонів ваг для цієї конкретної мережі вистачило для того, щоб просто запам'ятати випадкові дані

0:18:27.820,0:18:34.379
що піднімає питання про те, чи мінімізація цієї функції вартості насправді відповідає будь-якій структурі зображення,

0:18:34.380,0:18:36.380
чи це просто - ну, 

0:18:36.520,0:18:37.420
запам'ятовування. (Ліша) - Запам'ятати весь

0:18:37.420,0:18:43.859
набір даних про те, що таке правильна класифікація, і згодом - десь за півроку - на цьогорічній ICML 

0:18:44.470,0:18:49.039
була, скажімо, не зовсім спростувальна робота - робота, присвячена деяким аспектам на кшталт 

0:18:49.470,0:18:55.279
"гей, направді ці мережі роблять щось трохи розумніше". Якщо подивитися на цю криву точності

0:18:55.279,0:18:57.499
якби ви тренувалися лише на

0:18:58.259,0:19:05.179
випадковому наборі даних, ця крива йшла би вниз дуже повільно, майже лінійно

0:19:05.179,0:19:09.589
Таким чином, ви стараєтеся знайти локальний мінімум

0:19:09.590,0:19:15.289
правильних ваг, що дало б вам таку точність, тоді як якщо б ви насправді тренувались на структурованому наборі даних з 

0:19:15.289,0:19:21.439
правильним маркуванням, на початку ви б трохи заплутались, але потім дуже швидко рвонули, щоб дійти до цього

0:19:22.200,0:19:26.149
рівня точності, і тому в деякому сенсі було легше знайти

0:19:26.759,0:19:33.949
локальний максимум. Що ще було цікавого про це - висвітлилася ще одна робота кількарічної давності

0:19:34.080,0:19:36.080
що має набагато більше

0:19:36.990,0:19:39.169
спрощень щодо мережевих шарів

0:19:39.169,0:19:46.788
Але одним з результатів було те, що якщо подивитися на оптимізаційний ландшафт, то локальні мінімуми, які ці мережі, як правило, вивчають

0:19:47.340,0:19:54.079
є насправді однакової якості, тому в певному сенсі, якщо ваш набір даних є структурою, вам це видасться набагато простішим

0:19:58.139,0:20:01.189
(3B1B) - Дякую, як завжди, тим, хто підтримує мене на патреоні

0:20:01.190,0:20:06.950
Я раніше говорив, наскільки патреон змінює правила гри, але ці відео дійсно не були б можливими без вас.

0:20:07.230,0:20:12.889
Також хочу особливо подякувати Аmplifу Рartners за підтримку початкових роликів у цій серії

0:20:13.470,0:20:17.149
Вони зосереджуються на компаніях, пов'язаних з машинним навчанням на дуже ранніх стадіях та зі штучним інтелектом

0:20:17.309,0:20:19.110
І я впевнений у ймовірності того, що 

0:20:19.110,0:20:23.899
хтось із вас, хто дивиться це, і навіть більш ймовірно - хтось із ваших знайомих

0:20:24.210,0:20:27.949
прямо зараз знаходиться на етапі запуску такої компанії

0:20:28.230,0:20:31.279
Аmplifу Рartners із задоволенням поспілкувалися б із такими засновниками.

0:20:31.279,0:20:37.069
Вони навіть налаштували електронну пошту для цього відео, щоб ви могли зв’язатися з ними через

0:20:37.590,0:20:39.590
3blue1brown@amplifypartners.com
